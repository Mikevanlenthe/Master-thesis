Article: trump-twitter

Tokenized by nltk.word_tokenize 
Token analysis before pre-processing 
Raw number of tokens: 3
Raw number of types: 3
Raw Type token ratio (higher = more diversity in language use): 1.0

100 most freq tokens before (pre)processing: 
[('<', 167), ('>', 167), ('the', 156), (',', 155), ('of', 121), ('in', 76), ('nobarlabels', 69), ('a', 57), ('and', 55), ('.', 55), ('to', 54), ('bar', 49), ('/bar', 49), ('that', 48), ('’', 47), ('percent', 36), ('than', 33), ('more', 28), ('s', 25), ('films', 25), ('for', 24), ('is', 23), ('was', 22), ('on', 20), ('women', 20), ('at', 18), ('are', 16), ('(', 16), (')', 16), ('with', 16), ('about', 16), ('other', 15), ('—', 14), ('it', 14), ('have', 14), ('those', 13), ('were', 13), ('we', 13), ('all', 12), ('but', 12), ('as', 12), ('only', 12), ('drivers', 11), ('by', 11), ('from', 11), ('found', 11), ('one', 11), ('subjects', 11), ('data', 10), ('t', 10), ('any', 10), ('had', 10), ('$', 10), ('median', 10), ('“', 10), ('”', 10), ('who', 10), ('people', 10), ('color', 10), ('among', 9), ('average', 8), ('be', 8), ('still', 8), ('or', 8), ('between', 8), ('boomers', 8), ('use', 8), ('white', 8), (':', 7), ('each', 7), ('don', 7), ('over', 7), ('most', 7), ('less', 7), ('they', 7), ('test', 7), ('high', 7), ('days', 7), ('just', 7), ('per', 7), ('three', 6), ('where', 6), ('number', 6), ('state', 6), ('much', 6), ('insurance', 6), ('country', 6), ('national', 6), ('while', 6), ('has', 6), ('compared', 6), ('when', 6), ('an', 6), ('film', 6), ('this', 6), ('year', 6), ('been', 6), ('i', 5), ('some', 5), ('how', 5)]

Applied pre-processing:
Lowercased all tokens 
Tokens below minimum token length 2 filtered out
Punctuation filtered out ['!', '"', '#', '$', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '=', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '’', '”', '—', "'", '<', '>']
Words filtered out: ['facebooktwitteremail', 'nobarlabels', 'nolinelabels', 'nolabels']

Applied filters: 
bigrams = False
bothlabels = False
lemmatize = False
minimumtokenlength = 2
onlybarlabels = True
onlylinelabels = False
showuniquepostagtokens = True
stemmer = False
stopwords = False
trigrams = False

Token analysis after pre-processing 
Number of tokens: 0
Number of types: 0
Type token ratio: 0

Used nouns, verbs and adjectives in article: 
(tokens separated by , ) 
100 most freq tokens after processing: 
[('the', 156), ('of', 121), ('bar', 98), ('in', 76), ('and', 55), ('to', 54), ('that', 48), ('percent', 37), ('than', 33), ('more', 28), ('films', 25), ('is', 24), ('for', 24), ('was', 22), ('women', 21), ('on', 20), ('at', 18), ('are', 17), ('with', 16), ('about', 16), ('other', 15), ('it', 14), ('have', 14), ('those', 13), ('were', 13), ('we', 13), ('all', 12), ('but', 12), ('as', 12), ('only', 12), ('drivers', 11), ('by', 11), ('from', 11), ('found', 11), ('one', 11), ('subjects', 11), ('data', 10), ('any', 10), ('had', 10), ('median', 10), ('who', 10), ('people', 10), ('color', 10), ('average', 9), ('among', 9), ('be', 8), ('most', 8), ('still', 8), ('or', 8), ('between', 8), ('boomers', 8), ('use', 8), ('days', 8), ('white', 8), ('each', 7), ('country', 7), ('don', 7), ('over', 7), ('less', 7), ('they', 7), ('year', 7), ('test', 7), ('high', 7), ('just', 7), ('per', 7), ('three', 6), ('where', 6), ('number', 6), ('state', 6), ('much', 6), ('insurance', 6), ('national', 6), ('while', 6), ('has', 6), ('compared', 6), ('when', 6), ('an', 6), ('film', 6), ('this', 6), ('50', 6), ('respondents', 6), ('been', 6), ('some', 5), ('how', 5), ('there', 5), ('involved', 5), ('same', 5), ('so', 5), ('21', 5), ('like', 5), ('released', 5), ('budget', 5), ('lower', 5), ('sample', 5), ('least', 5), ('our', 5), ('school', 5), ('said', 5), ('also', 5), ('used', 5)]
