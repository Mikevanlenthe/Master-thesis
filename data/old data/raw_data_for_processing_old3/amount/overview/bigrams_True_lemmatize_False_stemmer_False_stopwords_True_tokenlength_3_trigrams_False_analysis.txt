Tokenized by nltk.word_tokenize 
Token analysis before pre-processing 
Raw number of tokens: 12
Raw number of types: 11
Raw Type token ratio: 0.9166666666666666

Applied pre-processing:
Lowercased all tokens 
Tokens below minimum length: 3 filtered out

Punctuation filtered out ['!', '"', '#', '$', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '’', '”', '—', "'"]
Words filtered out: ['facebooktwitteremail', '']

Applied filters:
Using stopwords filter = True
Using lemmatizer = False
Using stemmer = False

Token analysis after pre-processing 
Number of tokens: 22
Number of types: 21
Type token ratio: 0.9545454545454546

Used nouns, verbs and adjectives in article: 
(tokens separated by whitespace) 
('NNS', 'articles files readmefiles')

('VBD', 'scraped found found articlesscraped found109')

('NN', 'readme article url scraped115 sfound found128')

('JJ', 'articleurl urls')

