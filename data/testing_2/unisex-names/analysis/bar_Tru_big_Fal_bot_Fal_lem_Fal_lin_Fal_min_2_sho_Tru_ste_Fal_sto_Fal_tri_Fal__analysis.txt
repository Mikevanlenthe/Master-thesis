Article: unisex-names

Tokenized by nltk.word_tokenize 
Token analysis before pre-processing 
Raw number of tokens: 3
Raw number of types: 3
Raw Type token ratio (higher = more diversity in language use): 1.0

100 most freq tokens before (pre)processing: 
[(',', 44), ('<', 32), ('>', 32), ('the', 31), ('in', 21), ('of', 18), ('nobarlabels', 12), ('.', 11), ('drivers', 11), ('bar', 10), ('/bar', 10), ('to', 9), ('that', 9), ('a', 9), ('on', 8), ('percent', 7), ('and', 7), ('than', 6), ('’', 6), ('insurance', 6), ('for', 6), ('$', 6), ('were', 5), ('where', 5), ('are', 5), ('state', 5), ('(', 5), (')', 5), ('average', 5), ('crashes', 4), ('was', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('—', 4), ('all', 3), ('s', 3), ('car', 3), ('each', 3), ('out', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('with', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('only', 2), ('while', 2), ('have', 2), ('i', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('across', 2), ('country', 2), ('at', 2), ('texas', 2), ('don', 2), ('t', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('more', 2), ('north', 2), ('every', 2), ('latest', 2), ('far', 2), ('figures', 2), ('be', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010.', 2), ('still', 2), ('they', 2), ('show', 2), ('politics', 2)]

Applied pre-processing:
Lowercased all tokens 
Tokens below minimum token length 2 filtered out
Punctuation filtered out ['!', '"', '#', '$', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '=', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '’', '”', '—', "'", '<', '>']
Words filtered out: ['facebooktwitteremail', 'nobarlabels', 'nolinelabels', 'nolabels']

Applied filters: 
barlabelsonly = True
bigrams = False
bothlabels = False
lemmatize = False
linelabelsonly = False
minimumtokenlength = 2
showuniquepostagtokens = True
stemmer = False
stopwords = False
trigrams = False

Token analysis after pre-processing 
Number of tokens: 0
Number of types: 0
Type token ratio: 0

Used nouns, verbs and adjectives in article: 
(tokens separated by , ) 
100 most freq tokens after processing: 
[('the', 31), ('in', 21), ('bar', 20), ('of', 18), ('drivers', 11), ('to', 9), ('that', 9), ('on', 8), ('percent', 7), ('and', 7), ('than', 6), ('insurance', 6), ('for', 6), ('were', 5), ('where', 5), ('are', 5), ('state', 5), ('average', 5), ('crashes', 4), ('was', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('all', 3), ('car', 3), ('each', 3), ('out', 3), ('country', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('with', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('year', 2), ('only', 2), ('while', 2), ('have', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('across', 2), ('at', 2), ('texas', 2), ('don', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('more', 2), ('north', 2), ('every', 2), ('2011', 2), ('latest', 2), ('far', 2), ('figures', 2), ('be', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010', 2), ('still', 2), ('they', 2), ('show', 2), ('politics', 2), ('or', 2), ('an', 2), ('candidate', 2), ('polls', 2), ('voters', 2), ('hand', 1), ('coasted', 1), ('1989', 1), ('2014', 1), ('during', 1), ('span', 1), ('clinton', 1), ('her', 1)]
