Article: nba-tattoos(hier)

Tokenized by nltk.word_tokenize 
Token analysis before pre-processing 
Raw number of tokens: 3
Raw number of types: 3
Raw Type token ratio (higher = more diversity in language use): 1.0

100 most freq tokens before (pre)processing: 
[(',', 40), ('the', 28), ('<', 21), ('>', 21), ('in', 20), ('of', 13), ('.', 11), ('drivers', 11), ('that', 9), ('bar', 8), ('to', 8), ('/bar', 8), ('on', 7), ('a', 7), ('and', 6), ('’', 6), ('insurance', 6), ('$', 6), ('nobarlabels', 5), ('than', 5), ('were', 5), ('state', 5), ('(', 5), (')', 5), ('for', 5), ('average', 5), ('percent', 4), ('where', 4), ('are', 4), ('crashes', 4), ('was', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('—', 4), ('all', 3), ('s', 3), ('car', 3), ('each', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('with', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('only', 2), ('while', 2), ('have', 2), ('i', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('out', 2), ('across', 2), ('country', 2), ('at', 2), ('texas', 2), ('don', 2), ('t', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('north', 2), ('every', 2), ('latest', 2), ('far', 2), ('figures', 2), ('be', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010.', 2), ('still', 2), ('they', 2), ('show', 2), ('politics', 2), ('hand', 1)]

Applied pre-processing:
Lowercased all tokens 
Tokens below minimum token length 2 filtered out
Punctuation filtered out ['!', '"', '#', '$', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '=', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '’', '”', '—', "'", '<', '>']
Words filtered out: ['facebooktwitteremail', 'nobarlabels', 'nolinelabels', 'nolabels']

Applied filters: 
barlabelsonly = True
bigrams = False
bothlabels = False
lemmatize = False
linelabelsonly = False
minimumtokenlength = 2
showuniquepostagtokens = True
stemmer = False
stopwords = False
trigrams = False

Token analysis after pre-processing 
Number of tokens: 0
Number of types: 0
Type token ratio: 0

Used nouns, verbs and adjectives in article: 
(tokens separated by , ) 
100 most freq tokens after processing: 
[('the', 28), ('in', 20), ('bar', 16), ('of', 13), ('drivers', 11), ('that', 9), ('to', 8), ('on', 7), ('and', 6), ('insurance', 6), ('than', 5), ('were', 5), ('state', 5), ('for', 5), ('average', 5), ('percent', 4), ('where', 4), ('are', 4), ('crashes', 4), ('was', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('all', 3), ('car', 3), ('each', 3), ('country', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('with', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('year', 2), ('only', 2), ('while', 2), ('have', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('out', 2), ('across', 2), ('at', 2), ('texas', 2), ('don', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('north', 2), ('every', 2), ('2011', 2), ('latest', 2), ('far', 2), ('figures', 2), ('be', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010', 2), ('still', 2), ('they', 2), ('show', 2), ('politics', 2), ('hand', 1), ('coasted', 1), ('1989', 1), ('2014', 1), ('during', 1), ('span', 1), ('clinton', 1), ('her', 1), ('single', 1), ('overall', 1), ('13', 1), ('60', 1), ('been', 1), ('positive', 1)]
