Article: terrorism

Tokenized by nltk.word_tokenize 
Token analysis before pre-processing 
Raw number of tokens: 3
Raw number of types: 3
Raw Type token ratio (higher = more diversity in language use): 1.0

100 most freq tokens before (pre)processing: 
[(',', 36), ('the', 23), ('in', 19), ('<', 17), ('>', 17), ('of', 12), ('drivers', 11), ('.', 9), ('bar', 7), ('that', 7), ('/bar', 7), ('to', 6), ('insurance', 6), ('$', 6), ('on', 5), ('than', 5), ('and', 5), ('’', 5), ('state', 5), ('(', 5), (')', 5), ('for', 5), ('average', 5), ('where', 4), ('are', 4), ('crashes', 4), ('a', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('nobarlabels', 3), ('were', 3), ('car', 3), ('each', 3), ('was', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('only', 2), ('percent', 2), ('all', 2), ('while', 2), ('i', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('s', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('out', 2), ('across', 2), ('country', 2), ('at', 2), ('texas', 2), ('don', 2), ('t', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('north', 2), ('every', 2), ('latest', 2), ('—', 2), ('far', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010.', 2), ('still', 2), ('hand', 1), ('coasted', 1), ('1989', 1), ('2014', 1), ('with', 1), ('during', 1), ('span', 1), ('clinton', 1)]

Applied pre-processing:
Lowercased all tokens 
Tokens below minimum token length 2 filtered out
Punctuation filtered out ['!', '"', '#', '$', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '=', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '’', '”', '—', "'", '<', '>']
Words filtered out: ['facebooktwitteremail', 'nobarlabels', 'nolinelabels', 'nolabels']

Applied filters: 
barlabelsonly = True
bigrams = False
bothlabels = False
lemmatize = False
linelabelsonly = False
minimumtokenlength = 2
showuniquepostagtokens = True
stemmer = False
stopwords = False
trigrams = False

Token analysis after pre-processing 
Number of tokens: 0
Number of types: 0
Type token ratio: 0

Used nouns, verbs and adjectives in article: 
(tokens separated by , ) 
100 most freq tokens after processing: 
[('the', 23), ('in', 19), ('bar', 14), ('of', 12), ('drivers', 11), ('that', 7), ('to', 6), ('insurance', 6), ('on', 5), ('than', 5), ('and', 5), ('state', 5), ('for', 5), ('average', 5), ('where', 4), ('are', 4), ('crashes', 4), ('collisions', 4), ('involved', 4), ('national', 4), ('were', 3), ('car', 3), ('each', 3), ('was', 3), ('country', 3), ('is', 3), ('but', 3), ('million', 3), ('billion', 3), ('miles', 3), ('traveled', 3), ('trump', 2), ('other', 2), ('from', 2), ('fewer', 2), ('negative', 2), ('references', 2), ('had', 2), ('worst', 2), ('only', 2), ('percent', 2), ('all', 2), ('while', 2), ('your', 2), ('three', 2), ('data', 2), ('america', 2), ('number', 2), ('those', 2), ('how', 2), ('much', 2), ('companies', 2), ('out', 2), ('across', 2), ('at', 2), ('texas', 2), ('don', 2), ('any', 2), ('them', 2), ('good', 2), ('according', 2), ('fatal', 2), ('north', 2), ('every', 2), ('2011', 2), ('latest', 2), ('far', 2), ('new', 2), ('most', 2), ('expensive', 2), ('collision', 2), ('idahoans', 2), ('best', 2), ('come', 2), ('costing', 2), ('insurers', 2), ('2010', 2), ('still', 2), ('hand', 1), ('coasted', 1), ('1989', 1), ('2014', 1), ('with', 1), ('during', 1), ('span', 1), ('clinton', 1), ('her', 1), ('single', 1), ('year', 1), ('overall', 1), ('13', 1), ('60', 1), ('have', 1), ('been', 1), ('positive', 1), ('want', 1), ('try', 1), ('answer', 1), ('question', 1), ('using', 1)]
