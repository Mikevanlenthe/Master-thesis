Sentence nr: 0
Sep. 25, 2014 at 1:54 PM How FiveThirtyEight Calculates Pollster Ratings By Nate Silver Filed under Polling Get the data on GitHub GitHub data at data/pollster-ratings FacebookTwitterEmail Illustration by Matt Chase Illustration by Matt Chase See FiveThirtyEight’s pollster ratings.

Sentence nr: 1
Pollster ratings were one of the founding features of FiveThirtyEight.

Sentence nr: 2
I was rating pollsters before I was building election models.

Sentence nr: 3
I was eagerly updating the ratings after every major batch of election results.

Sentence nr: 4
I rated pollsters while walking two miles uphill … barefoot … in the snow.

Sentence nr: 5
And then I got a little burned out on them.

Sentence nr: 6
We last issued a major set of pollster ratings in June 2010 and made only a cursory update before the 2012 elections.

Sentence nr: 7
What happened?

Sentence nr: 8
Well, when you publish a set of pollster ratings, people are understandably fixated upon how you’ve rated the individual polling firms: Is Pollster XYZ better than Pollster PDQ?

Sentence nr: 9
Naturally, we hope the pollster ratings can give you a better basis for understanding the polls as a news consumer.

Sentence nr: 10
However, discussions about individual polling firms — there are now more than 300 of them in our database — can sometimes miss the point.

Sentence nr: 11
I’m more interested in the big-picture questions.

Sentence nr: 12
Are some pollsters consistently better than others, as measured by how accurately they predict election results?

Sentence nr: 13
In other words, is pollster performance predictable?

Sentence nr: 14
And if so, are a pollster’s past results the better predictor — or are its methodological standards more telling?

Sentence nr: 15
The short answer is that pollster performance is predictable — to some extent.

Sentence nr: 16
Polling data is noisy and bad pollsters can get lucky.

Sentence nr: 17
But pollster performance is predictable on the scale of something like the batting averages of Major League Baseball players.

Sentence nr: 18
Let me take that analogy a bit further.

Sentence nr: 19
In baseball, there isn’t much difference in an absolute sense between a .300 hitter and a .260 hitter — it amounts to getting about one extra hit during each week of the baseball season.

Sentence nr: 20
Likewise, the differences in poll accuracy aren’t that large.

Sentence nr: 21
We estimate that the very best pollsters might be about 1 percentage point more accurate than the average pollster over the long run.

Sentence nr: 22
However, the average poll in our database missed the final election outcome by 5.3 percentage points.

Sentence nr: 23
That means even the best poll would still be off by 4.3 points.

Sentence nr: 24
It’s almost always better to take an average of polls rather than hoping for any one of them to “hit a bullet with a bullet.” What about the very worst pollsters?

Sentence nr: 25
Well, we estimate that the absolute worst ones might introduce 2 to 3 points of error, as compared with average polls, based on poor methodology.

Sentence nr: 26
That means that the worst polls are worse (further below average) than the best polls are good (above average).

Sentence nr: 27
While there are intrinsic limits to how accurate any poll can be (because of sampling error and other factors), there is no shortage of ways to screw up.

Sentence nr: 28
But just as most baseball players hit somewhere around .260, most pollsters tend to be about average.

Sentence nr: 29
Or at least, that’s the best guess we can make based on examining their past results.

Sentence nr: 30
Poll accuracy statistics, like batting averages, take a long time to converge to the mean.

Sentence nr: 31
You shouldn’t assume a polling firm is awesome just because it nailed the most recent election any more than you should mistake a shortstop who went 2-for-5 one day for a .400 hitter.

Sentence nr: 32
Nonetheless, when you aggregate results over a number of elections and the sample sizes become larger, you’ll find that there is some consistency in pollster performance.

Sentence nr: 33
Before we go any further, I’d encourage you to download the database of polls that we’ve used to construct the pollster ratings.

Sentence nr: 34
We’re making it public for the first time.

Sentence nr: 35
The database includes (with just a few minor exceptions that I’ll describe below) every poll conducted in the last three weeks of a presidential, U.S. Senate, U.S. House or gubernatorial campaign since 1998, along with polls in the final three weeks of presidential primaries and caucuses since 2000.

Sentence nr: 36
Test everything out for yourself — probably you’ll agree with some elements of our approach and disagree with others.

Sentence nr: 37
Better yet, maybe you’ll discover a bunch of cool things that we hadn’t thought to look for.

Sentence nr: 38
We think there should be more pollster ratings — FiveThirtyEight shouldn’t have the last word on them.

Sentence nr: 39
Perhaps the simplest measure of poll accuracy is how far the poll’s margin was from the actual election result.

Sentence nr: 40
For instance, if a poll had the Democrat ahead by 10 percentage points and she actually won by 3 points, that would represent a 7-point error.

Sentence nr: 41
In the table below, I’ve listed polling firms’ average error for elections from 1998 through 2007, and again for the same polling firms for elections from 2008 onward.

Sentence nr: 42
(About half the polls in our database are from 2008 or later, so this is a logical dividing point.)

Sentence nr: 43
I’ve restricted the list to the 28 firms with at least 10 polls in both halves of the sample.

Sentence nr: 44
As you can see, there’s a fair amount of consistency in these results; the correlation coefficient (where 1 is a perfect correlation and 0 is no correlation) is about 0.6.

Sentence nr: 45
InsiderAdvantage and American Research Group were among the least accurate pollsters in both halves of the sample; polls from ABC News and The Washington Post (who usually conduct polls jointly) were among the most accurate in both cases.

Sentence nr: 46
(ABC News, like ESPN and FiveThirtyEight, is owned by the Walt Disney Company.)

Sentence nr: 47
But there are a number of other things we’ll want to account for.

Sentence nr: 48
In particular, we’ll want to know how much of the error had to do with circumstantial factors.

Sentence nr: 49
For instance, polls of presidential primaries are associated with much more error than polls of general elections.

Sentence nr: 50
This is a consequence of factors intrinsic to primaries (for instance, turnout is far lower) and mostly isn’t the pollsters’ fault.

Sentence nr: 51
One more baseball analogy: Polling primaries is like hitting in Dodger Stadium against Clayton Kershaw, whereas polling general elections is like hitting in Coors Field.

Sentence nr: 52
How do we account for factors like these?

Sentence nr: 53
It takes some work — the balance of this article will be devoted to describing our process.

Sentence nr: 54
This year, we’re publishing a variety of different versions of the pollster ratings that range from simple to more complex.

Sentence nr: 55
If at any point you think we’ve made one assumption too many, you can take the exit ramp and use one of the simpler versions.

Sentence nr: 56
Or you can download the raw data and construct your own.

Sentence nr: 57
Our overall method is largely the same as in 2010.

Sentence nr: 58
That year, for the first time, we introduced a consideration of a poll’s methodological standards in addition to its past accuracy.

Sentence nr: 59
We think the case for doing so has probably grown stronger since then, but you can find a number of versions of the pollster ratings based on past accuracy alone if you prefer them.

Sentence nr: 60
There are also a few things I’ve come to think about differently since 2010.

Sentence nr: 61
First, the case against Internet-based polls has grown much weaker in the last four years.

Sentence nr: 62
At that time, the most prominent Internet pollster was Zogby Interactive (it has since been re-branded as JZ Analytics), which used a poor methodology and got equally poor results.

Sentence nr: 63
But Internet penetration has increased considerably since then (it now exceeds landline telephone penetration) and a number of Internet-based polling firms with more thoughtful methodologies have come along.

Sentence nr: 64
In 2012, the Internet-based polls did a little better than the telephone polls as a group (especially compared to telephone polls that did not call cellphones).

Sentence nr: 65
There are still some reasons to be skeptical of Internet polls — especially those that do not use probability sampling.

Sentence nr: 66
But the FiveThirtyEight pollster ratings no longer include an explicit penalty for Internet polls1 as they did in 2010.

Sentence nr: 67
Second, it has become harder to distinguish a “partisan” poll.

Sentence nr: 68
As I described earlier this month, FiveThirtyEight has been applying a more relaxed standard for what we define as “partisan” polls since 2012.

Sentence nr: 69
The challenge in trying to use a more restrictive standard had been that there were too many borderline cases — and we didn’t like having to make a lot of ad hoc decisions about which polls to include.

Sentence nr: 70
Some polling firms, like Public Policy Polling, conduct polls on behalf of interest groups and campaigns but pay for others themselves.

Sentence nr: 71
Blogs like Daily Kos and Red Racing Horses now sponsor polls.

Sentence nr: 72
And in some cases, it isn’t clear who’s paying for a poll.

Sentence nr: 73
Only the most unambiguously partisan polls — those sponsored by candidates or by party groups like the Republican National Committee — are excluded from the FiveThirtyEight forecast models.

Sentence nr: 74
But we still keep track of polls even when we don’t use them in our forecast models — and their results are reflected in the pollster ratings.

Sentence nr: 75
These polls are labeled with a partisan “flag” in the database.2 The idea is that a polling firm ought to be held accountable for any poll it puts out for public consumption.

Sentence nr: 76
If a polling firm releases biased and inaccurate polls on behalf of candidates, that will be reflected in its pollster rating — even if it does better work when conducting polls on behalf of a media organization.

Sentence nr: 77
Our pollster ratings database also includes a couple of ways for you to track potential bias in the polls.

Sentence nr: 78
The term bias itself refers to how much a polling firm’s results have erred toward one party or the other as compared against actual election results.

Sentence nr: 79
House effect, by contrast, refers to how a firm’s results compare against other polls.

Sentence nr: 80
If Pollster PDQ had the Democrat ahead by 5 points in an election where every other pollster had the race tied, it would have a Democratic house effect.

Sentence nr: 81
But if the Democrat turned out to win by 10 points, PDQ would have a Republican bias as compared against the actual election results.

Sentence nr: 82
As is the case for measures of poll accuracy, measures of bias and house effects can sometimes reflect statistical noise rather than anything systematic.

Sentence nr: 83
But if they occur over dozens or hundreds of surveys, they should be a concern.

Sentence nr: 84
Third, we’re seeing clearer evidence of pollster “herding.” Herding is the tendency of some polling firms to be influenced by others when issuing poll results.

Sentence nr: 85
A pollster might want to avoid publishing a poll if it perceives that poll to be an outlier.

Sentence nr: 86
Or it might have a poor methodology and make ad hoc adjustments so that its poll is more in line with a stronger one.

Sentence nr: 87
The problem with herding is that it reduces polls’ independence.

Sentence nr: 88
One benefit of aggregating different polls is that you can account for any number of different methods and perspectives.

Sentence nr: 89
But take the extreme case where there’s only one honest pollster in the field and a dozen herders who look at the honest polling firm’s results to calibrate their own.

Sentence nr: 90
(For instance, if the honest poll has the Democrat up by 6 points, perhaps all the herders will list the Democrat as being ahead by somewhere between 4 and 8 points.)

Sentence nr: 91
In this case, you really have just one poll that provides any information — everything else is just a reflection of its results.

Sentence nr: 92
And if the honest poll happens to go wrong, so will everyone else’s results.3 There’s reasonably persuasive evidence that herding has occurred in polls of Senate elections, presidential primaries and the most recent presidential general election.

Sentence nr: 93
It seems to be more common among pollsters that take methodological shortcuts.

Sentence nr: 94
Paradoxically, while herding may make an individual polling firm’s results more accurate, it can make polling averages worse.

Sentence nr: 95
There’s some tentative evidence that this is already happening.

Sentence nr: 96
From our polling database, I compared two quantities: First, how accurate the average individual poll was; and second, how accurate the polling average was.4 I limited the analysis to general election races where at least three polls had been conducted.

Sentence nr: 97
From 1998 through 2007, the average poll in these races missed the final margin by 4.7 percentage points.

Sentence nr: 98
The average error has been somewhat lower — 4.3 percentage points — in races from 2008 onward.

Sentence nr: 99
But the polling average hasn’t gotten any better — if anything it’s gotten slightly worse.

Sentence nr: 100
From 1998 through 2007, the polling average missed the final margin in an election by an average of 3.7 percentage points.

Sentence nr: 101
Since 2008, the error has been 3.9 percentage points instead.

Sentence nr: 102
So this is something we’re concerned about — the benefit of aggregating polls together will decline if herding behavior continues to increase.

Sentence nr: 103
This year’s pollster ratings introduce a couple of attempts to account for such behavior.

Sentence nr: 104
Now let’s get into the details — what follows is a reasonably comprehensive description of how we calculate the pollster ratings.

Sentence nr: 105
Step 1: Collect and classify polls Almost all of the work is in this step; we’ve spent hundreds of hours over the years collecting polls.

Sentence nr: 106
The ones represented in the pollster ratings database meet three simple criteria: They were conducted in 1998 or later; They were conducted in the final three weeks of the campaign; They were conducted in one of the following types of elections: Presidential general elections; Presidential primaries; Senate elections; Gubernatorial elections; U.S. House elections.

Sentence nr: 107
Of course, it’s not quite that simple; a number of other considerations come up from time to time: Sample sizes are sometimes missing from older polls.

Sentence nr: 108
In these cases, we’ve estimated a poll’s sample size from its reported margin of error or from how many people a polling firm surveyed in other polls where the sample size was listed.5 As a last resort, we use 600 as a default sample size.

Sentence nr: 109
If a pollster listed results among likely voters and registered voters (or all adults), we list only the likely voter version in the database.

Sentence nr: 110
Because the database covers the final three weeks of the campaign and almost all polling firms publish likely voter polls by that time, almost all polls in the database should be likely voter surveys.

Sentence nr: 111
When a pollster publishes multiple versions of the same survey (for example, versions of the poll with and without a third-party candidate included), FiveThirtyEight’s policy is to average the versions together.

Sentence nr: 112
However, some of the polls in our database were taken from sources that may have followed different rules, so the treatment of these cases may be inconsistent.

Sentence nr: 113
Polls of special elections are included.

Sentence nr: 114
Polls of nonpartisan primaries (such as in Louisiana) are included.6 National polls for the presidential popular vote and the generic congressional ballot are included.7 The use of tracking polls is restricted to non-overlapping dates.

Sentence nr: 115
For instance, if a firm’s final tracking poll was conducted on the Friday through the Sunday before an election, we wouldn’t also list the version that covered Thursday through Saturday.

Sentence nr: 116
Polls are included in the database even if they were not used in the FiveThirtyEight forecasts.8 A poll’s date as listed in the database reflects the median date the poll was in the field — not the date the poll was released.

Sentence nr: 117
For example, a poll conducted from Oct. 20 to Oct. 22 and released on Oct. 25 would have its date listed as Oct. 21.

Sentence nr: 118
Although in general all polls within the final three weeks of a campaign are included, there are minor exceptions in the case of the presidential primaries.

Sentence nr: 119
No polls of the New Hampshire primary are included until after the Iowa caucus has been completed, and no polls of states beyond New Hampshire are included until New Hampshire has voted.9 Sources for the data include previous iterations of FiveThirtyEight, along with HuffPost Pollster, Real Clear Politics, PollingReport.com, the Internet Archive, and searches of Google News and other newspaper archives.

Sentence nr: 120
They also include data sent to us by various polling firms — however, we have sought to verify that such polls were in fact released to the public in advance of each election10 and that the pollster did not cherry-pick the results sent to us.

Sentence nr: 121
We’ve chosen 1998 as the cutoff point because there are multiple sources covering that election onward, meaning that the data ought to be reasonably comprehensive.

Sentence nr: 122
Nevertheless, we’re certain that there are omissions from the database.

Sentence nr: 123
We’re equally certain that there are any number of errors — some that were included in the original sources, and some that we’ve introduced ourselves.

Sentence nr: 124
We’re hoping that releasing the data publicly will allow people to check for potential errors and omissions.11 A big challenge comes in how to identify the pollster we associate with each survey.

Sentence nr: 125
For instance, Marist College has recently begun to conduct polls for NBC News.

Sentence nr: 126
Are these classified as Marist College polls, NBC News polls, NBC/Marist polls, or something else?

Sentence nr: 127
The answer is that they’re Marist College polls.

Sentence nr: 128
Our policy is to classify the poll with the pollster itself rather than the media sponsor.

Sentence nr: 129
However, a few media companies like CBS News and The New York Times have in-house polling operations.12 Confusingly, media companies sometimes also act as the sponsors of polls conducted by other firms.

Sentence nr: 130
Our goal is to associate the poll with the company that, in our estimation, contributed the most intellectual property to the survey’s methodology.

Sentence nr: 131
For instance, the set of polls conducted earlier this year by YouGov for CBS News and The New York Times are classified as YouGov polls, not CBS News/New York Times polls.13 Polling firms sometimes operate under multiple brand names and add or subtract partners.

Sentence nr: 132
Some cases are reasonably clear — for instance, Rasmussen Reports is a subsidiary of Pulse Opinion Research, so polls marketed under each name are classified together.

Sentence nr: 133
Other cases are more ambiguous; we’ve simply had to apply our best judgment about where one polling firm ends and another begins.

Sentence nr: 134
In previous versions of the pollster ratings, we included separate entries for telephone and Internet polls from the same company — for instance, Ipsos conducts both types of polls and they’re listed separately in the database.

Sentence nr: 135
This is becoming increasingly impractical as polling firms adopt mixed-mode samples (polls with Internet and telephone responses combined together) or otherwise fail to clearly differentiate one mode from the other.

Sentence nr: 136
For now, we have grandfathered in preexisting cases like Ipsos and continued to list their Internet and telephone polls as separate entries.

Sentence nr: 137
However, this will very likely change with the next major release of the pollster ratings database after the 2014 election.

Sentence nr: 138
Step 2: Calculate simple average error This part’s really simple: We compare the margin in the poll against the election result and see how far apart they were.

Sentence nr: 139
If the poll projected the Republican to win by 4 points and he won by 9 instead, that reflects a 5-point error.

Sentence nr: 140
(Our preferred source for election results is Dave Leip’s Atlas of U.S. Presidential Elections.)

Sentence nr: 141
The error is calculated based on the margin separating the top two finishers in the election — and not the top two candidates in the poll.

Sentence nr: 142
For instance, if a certain poll had the 2008 Iowa Democratic caucus with Hillary Clinton at 32 percent, Barack Obama with 30 percent and John Edwards with 28 percent, we’d look at how much it projected Obama to win over Edwards since they were the top two finishers (Clinton narrowly finished third).

Sentence nr: 143
The database also includes a column indicating whether a poll “called” the winner of the race correctly.

Sentence nr: 144
But we think this is generally a poor measure of poll accuracy.

Sentence nr: 145
In a race that the Democrat won by 1 percentage point, a poll that had the Republican winning by 1 point did a pretty good job, whereas one that had the Democrat winning by 13 was wildly off the mark.

Sentence nr: 146
Step 3: Calculate Simple Plus-Minus As I mentioned, some elections are more conducive to accurate polling.

Sentence nr: 147
In particular, presidential general elections are associated with accurate polling while presidential primaries are much more challenging to poll.14 Polls of general elections for Congress and for governor are somewhere in between.

Sentence nr: 148
This step seeks to account for that consideration along with a couple of other factors.

Sentence nr: 149
We run a regression analysis that predicts polling error based on the type of election surveyed,15 a poll’s sample size,16 and the number of days17 separating the poll from the election.18 We then calculate a plus-minus score by comparing a poll’s average error against the error one would expect from these factors.

Sentence nr: 150
For instance, Quinnipiac University polls have an average error of 4.6 percentage points.

Sentence nr: 151
By comparison, the average pollster, surveying the same types of races on the same dates and with the same sample sizes, would have an error of 5.3 points according to the regression.

Sentence nr: 152
Quinnipiac therefore gets a Simple Plus-Minus score of -0.7.

Sentence nr: 153
This is a good score: As in golf, negative scores indicate better-than-average performance.

Sentence nr: 154
Specifically, it means Quinnipiac polls have been 0.7 percentage points more accurate than other polls under similar circumstances.

Sentence nr: 155
A few words about the other factors Simple Plus-Minus considers: In the past, we’ve described the error in polls as resulting from three major components: sampling error, temporal error and pollster-induced error.

Sentence nr: 156
They are related by a sum of squares formula: \[Total\ Error=\sqrt{Sampling\ Error + Temporal\ Error + Pollster\text{-}Induced\ Error}\] Sampling error reflects the fact that a poll surveys only some portion of the electorate rather than everybody.

Sentence nr: 157
This matters less than you might expect; a poll of 1,000 voters will miss the final margin in the race by an average of only about 2.5 percentage points because of sampling error alone — even in a state with 10 million voters.19 Unfortunately, sampling error isn’t the only problem pollsters have to worry about.

Sentence nr: 158
Another concern is that polls are (almost) never conducted on Election Day itself.

Sentence nr: 159
I refer to this property as temporal (or time-dependent) error.

Sentence nr: 160
There have been elections when important news events occurred in the 48 to 72 hours that separated the final polls from the election, such as the New Hampshire Democratic primary debate in 2008, or the revelation of George W. Bush’s 1976 DUI arrest before the 2000 presidential election.

Sentence nr: 161
If late-breaking news can sometimes affect the outcome of elections, why go back three weeks in evaluating pollster accuracy?

Sentence nr: 162
Well, there are a number of considerations we need to balance against the possibility of last-minute shifts in the polls: The overwhelming majority of elections do not feature important late-breaking developments.

Sentence nr: 163
There will often be head-fakes and media-hyped “game changers,” but they rarely make much difference upon careful analysis.

Sentence nr: 164
Herding (see above) becomes more prominent in the final few days before an election.

Sentence nr: 165
It’s fairly common for a pollster to publish some wild-seeming results — which can affect media coverage of the campaign — only to “fall in line” with its final poll.

Sentence nr: 166
Some of the apparent movement in the polls in the late days of the election is probably artificial, reflecting response bias (voters for a certain candidate might be more likely to respond to polls after the candidate has a strong news cycle) and badly designed turnout models rather than genuine changes in public opinion.

Sentence nr: 167
“Election Day” is something of a misnomer.

Sentence nr: 168
Many states accept ballots by mail or provide for early voting; in the 2012 presidential election, about one-quarter of the votes nationwide were cast before Nov. 6.

Sentence nr: 169
Accounting for all polls in the final three weeks of the campaign increases the sample size.

Sentence nr: 170
Three weeks is an arbitrary cutoff point; I’d have no profound objection to expanding the interval to a month or narrowing it to two weeks, or to using a slightly different standard for primaries and general elections.

Sentence nr: 171
But we feel strongly that evaluating a polling firm’s accuracy based only on its very last poll is a mistake.

Sentence nr: 172
Nonetheless, the pollster ratings account for the fact that polling on the eve of the election is slightly easier than doing so a couple of weeks out.

Sentence nr: 173
So a firm shouldn’t be at any advantage or disadvantage because of when it surveys a race.

Sentence nr: 174
The final component is what we’ve referred to in the past as pollster-induced error; it’s the residual error component that can’t be explained by sampling error or temporal error.

Sentence nr: 175
I’ve grown to dislike the term “pollster-induced error”; it sounds more accusatory than it should.

Sentence nr: 176
Certain things (like projecting turnout) are inherently pretty hard and it may not be the pollster’s fault when it fails to do them perfectly.

Sentence nr: 177
Our research suggests that even if all polls were conducted on Election Day itself (no temporal error) and took an infinite sample size (no sampling error) the average one would still miss the final margin in the race by about 2 percentage points.

Sentence nr: 178
However, some polling firms are associated with more of this type of error.

Sentence nr: 179
That’s what our plus-minus scores seek to evaluate.

Sentence nr: 180
Step 4: Calculate Advanced Plus-Minus Earlier this year, House majority leader Eric Cantor lost his Republican primary to David Brat, a college professor, in Virginia’s 7th congressional district.

Sentence nr: 181
It was a stunning upset, at least according to the polls.

Sentence nr: 182
For instance, a Vox Populi poll had put Cantor ahead by 12 points.

Sentence nr: 183
Instead, Brat won by 12 points.

Sentence nr: 184
The Vox Populi poll missed by 24 points.

Sentence nr: 185
According to Simple Plus-Minus, that poll would score very poorly.

Sentence nr: 186
We don’t have a comprehensive database of House primary polls and don’t include them in the pollster ratings, but I’d guess that such polls are off by something like 10 percentage points on average.

Sentence nr: 187
Vox Populi’s poll missed by 24, so it would get a Simple Plus-Minus score of +14.

Sentence nr: 188
That seems pretty terrible — until you compare it to the only other poll of the race, an internal poll released by McLaughlin & Associates on behalf of Cantor’s campaign.

Sentence nr: 189
That poll had Cantor up by 34 points — a 46-point error!

Sentence nr: 190
If we calculated something called Relative Plus-Minus (how the poll compares against others of the same race) the Vox Populi poll would get a score of -22, since it was 22 points more accurate than the McLaughlin survey.

Sentence nr: 191
Advanced Plus-Minus, the next step in the calculation, seeks to balance these considerations.

Sentence nr: 192
It weights Relative Plus-Minus based on the number of distinct polling firms20 that surveyed the same race, then treats Simple Plus-Minus as equivalent to three polls.

Sentence nr: 193
For example, if six other polling firms surveyed a certain race, Relative Plus-Minus would get two-thirds of the weight and Simple Plus-Minus would get one-third.

Sentence nr: 194
The short version: When there are a lot of polls in the field, Advanced Plus-Minus is mostly based on how well a poll did in comparison to others of the same election.

Sentence nr: 195
But when there is scant polling, it’s mostly based on a comparison to polls of the same type of election (for example, other presidential primaries).

Sentence nr: 196
Meticulous readers might wonder about another problem.

Sentence nr: 197
If we’re comparing a poll against its competitors, shouldn’t we account for the strength of the competition?

Sentence nr: 198
If a pollster misses every election by 40 points, it’s easy to look good by comparison if you happen to poll the same races.

Sentence nr: 199
The problem is similar to the one you’ll encounter if you try to design college football or basketball rankings: Ideally, you’ll want to account for strength of schedule in addition to wins and losses and margin of victory.

Sentence nr: 200
Advanced Plus-Minus addresses this by means of iteration (see a good explanation here), a technique commonly applied in sports power ratings.

Sentence nr: 201
Advanced Plus-Minus also addresses another problem.

Sentence nr: 202
As I’ve mentioned, polls tend to be more accurate when there are more of them in the field.

Sentence nr: 203
This may reflect herding, selection bias (pollsters may be more inclined to survey easier races; consider how many of them are avoiding the challenging Senate races in Kansas and Alaska this year), or some combination thereof.

Sentence nr: 204
So Advanced-Plus Minus also adjusts scores based on how many other polling firms surveyed the same election.

Sentence nr: 205
This has the effect of rewarding polling firms that survey races few other pollsters do and penalizing those that swoop in only after there are already a dozen polls in the field.

Sentence nr: 206
Two final wrinkles.

Sentence nr: 207
Advanced Plus-Minus puts slightly more weight on more recent polls.21 It also contains a subtle adjustment to account for the higher volatility of certain election types, especially presidential primaries.22 Before we proceed to the final step, let’s pause to re-examine the results for the 28 polling firms we listed before, but this time using Advanced Plus-Minus rather than Simple Average Error.

Sentence nr: 208
There’s still a correlation — although it’s somewhat weaker than before (the correlation coefficient is roughly 0.45 instead of 0.60).

Sentence nr: 209
Accounting for the fact that American Research Group polls a lot of primaries makes the firm look somewhat less bad, for instance.

Sentence nr: 210
But pollster performance still looks to be predictable to some extent.

Sentence nr: 211
As I’ll describe next, it’s more predictable if you look at a poll’s methodological standards in addition to its past performance.

Sentence nr: 212
Step 5: Calculate Predictive Plus-Minus When we last updated the pollster ratings in 2010, I failed to be explicit enough about our goal: to predict which polling firms would be most accurate going forward.

Sentence nr: 213
This is useful to know if you’re using polls to forecast election results, for example.

Sentence nr: 214
But that may not be your purpose.

Sentence nr: 215
If you’re interested in a purely retrospective analysis of poll accuracy, there are a number of measures of it in our pollster ratings spreadsheet.

Sentence nr: 216
For instance, you’ll find each pollster’s Simple Plus-Minus and Advanced Plus-Minus scores.

Sentence nr: 217
The version I’d personally recommend is called “Mean-Reverted Advanced Plus-Minus,” which is retrospective but discounts the results for pollsters with a small number of polls in the database.23 The difference with Predictive Plus-Minus is that it also accounts for a polling firm’s methodological standards — albeit in a slightly roundabout way.

Sentence nr: 218
In 2010, we looked at whether a polling firm was a member of the National Council on Public Polls (NCPP) or a supporter of the American Association for Public Opinion Research (AAPOR) Transparency Initiative.24 One other thing I was probably not clear enough about in 2010 was that participation in these organizations was intended as a proxy variable for methodological quality.

Sentence nr: 219
That is, it’s a correlate of methodological quality rather than a direct measure of it.25 Nevertheless, polling firms that participated in one of these initiatives tended to have more accurate polls prior to 2010.

Sentence nr: 220
Have they also been more accurate since?

Sentence nr: 221
Yes they have — and by a wide margin.

Sentence nr: 222
The chart below tracks the performance of polling firms based on whether they were members of NCPP or the AAPOR Transparency Initiative as of June 6, 2010, when FiveThirtyEight last released a full set of pollster ratings.26 From 1998 through 2009, the average poll from an AAPOR/NCPP polling firm had an error of 4.6 percentage points, compared with an average error of 5.5 percentage points for firms that did not participate in one of these groups.

Sentence nr: 223
While this difference is highly statistically significant, it isn’t that impressive.

Sentence nr: 224
The reason is that we evaluated participation in AAPOR/NCPP only after the fact.

Sentence nr: 225
Perhaps polling firms with terrible track records didn’t survive long enough to participate in AAPOR/NCPP as of June 2010, or perhaps AAPOR/NCPP didn’t admit them.

Sentence nr: 226
What is impressive is that the difference has continued to be just as substantial since June 2010.

Sentence nr: 227
In the general election in November 2010, polls from firms that had participated in AAPOR/NCPP as of that June were associated with an average error of 4.7 percentage points, compared with 5.7 percentage points for those that hadn’t.

Sentence nr: 228
And throughout 2012 (including both the presidential primaries and the general election), the AAPOR/NCPP polls were associated with an average error of 4.0 percentage points, versus 5.2 points for nonparticipants.

Sentence nr: 229
For clarity: The 2010 and 2012 results are true out-of-sample tests.

Sentence nr: 230
In the chart above, the polling firms are classified based on the way FiveThirtyEight had them in June 2010 — before these elections occurred.

Sentence nr: 231
In my view, this is reasonably persuasive evidence that methodology matters, at least to the extent we can infer something about it from AAPOR/NCPP participation.

Sentence nr: 232
This year, we’ve introduced a two-pronged test for methodological quality.

Sentence nr: 233
The first test is similar to before: Is a polling firm a member of NCPP, a participant in the AAPOR Transparency Initiative, or does it release its raw data to the Roper Center Archive?27 And second, does the firm regularly call cellphones in addition to landlines?

Sentence nr: 234
Each firm gets a methodological score between 0 and 2 based on the answers to these questions.

Sentence nr: 235
Tracking which firms call cellphones is tricky.

Sentence nr: 236
We’ve done a reasonably extensive search through recent polls to see whether they document calling cellphones.

Sentence nr: 237
However, we do not list a polling firm as calling cellphones until we have some evidence that it does.

Sentence nr: 238
There are undoubtedly some false negatives on our list; we encourage polling firms to contact us with documentation that they’ve been calling cellphones.28 So let’s say you have one polling firm that passes our methodological tests but hasn’t been so accurate, and another that doesn’t meet the methodological standards but has a reasonably good track record.

Sentence nr: 239
Which one should you expect to be more accurate going forward?

Sentence nr: 240
That’s the question Predictive Plus-Minus ratings are intended to address.

Sentence nr: 241
But the answer isn’t straightforward; it depends on how large a sample of polls you have from each firm.

Sentence nr: 242
Our finding is that past performance reflects more noise than signal until you have about 30 polls to evaluate, so you should probably go with the firm with the higher methodological standards up to that point.

Sentence nr: 243
If you have 100 polls from each pollster, however, you should tend to value past performance over methodology.29 One further complication is herding.

Sentence nr: 244
The methodologically inferior pollster may be posting superficially good results by manipulating its polls to match those of the stronger polling firms.

Sentence nr: 245
If left to its own devices — without stronger polls to guide it — it might not do so well.

Sentence nr: 246
My colleague Harry Enten looked at Senate polls since 2006 and found that methodologically poor pollsters improve their accuracy by roughly 2 percentage points when there are also strong polls in the field.

Sentence nr: 247
My own research on the broader polling database did not find quite so large an effect; instead it was closer to 0.6 percentage points.

Sentence nr: 248
Still, the effect was highly statistically significant.

Sentence nr: 249
As a result, Predictive Plus-Minus includes a “herding penalty” for pollsters with low methodology ratings.30 The formula for how to calculate Predictive Plus-Minus is included in the footnotes.31 Basically, it’s a version of Advanced Plus-Minus where scores are reverted toward a mean, where the mean depends on whether the poll passed one or both methodological standards.32 The fewer polls a firm has, the more its score is reverted toward this mean.

Sentence nr: 250
So Predictive Plus-Minus is mostly about a poll’s methodological standards for firms with only a few surveys in the database, and mostly about its past results for those with many.33 As a final step, we’ve translated each firm’s Predictive Plus-Minus rating into a letter grade, from A+ to F. One purpose of this is to make clear that the vast majority of polling firms cluster somewhere in the middle of the spectrum; about 84 percent of polling firms receive grades in the B or C range.

Sentence nr: 251
There are a whole bunch of other goodies in the pollster ratings spreadsheet, including various measures of bias and house effects.

Sentence nr: 252
We think the pollster ratings are a valuable tool, so we wanted to make sure you had a few more options for how to use them.

Sentence nr: 253
CORRECTION (May 21, 2016, 4 p.m.): An earlier version of this article included an incorrect date in the formula in Footnote 21.

Sentence nr: 254
The date should be 1988, not 1998.

