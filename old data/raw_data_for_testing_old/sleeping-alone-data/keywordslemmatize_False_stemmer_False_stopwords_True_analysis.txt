Token analysis before pre-processing 
Raw number of tokens: 1239
Raw number of types: 468
Raw Type token ratio: 0.37772397094430993

Filters:
Using stopwords filter = True
Using lemmatizer = False
Using stemmer = False

Used nouns, verbs and adjectives in article: 
(tokens separated by whitespace) 
('JJ', 'dear many separate melvin twin mona sleep different many anonymous anonymous carefuls careful scene ricky separate married couple onscreen high offscreen bedrooms anonymous many national sleep america sleep separate couch conducted detailed early surveymonkey american domestic civil significant slept curious first likely sleep possible marital different separate sleep separate anonymous surprising mentioned girlfriend affect slept anonymous lucy outdated percent sleep separate slept slept separate couch large different several several half decide sleep tried comprehensive possible high checked sleep illness anonymous different bizarre bedtime cindy overall sacrosanct separate optimistic evolution social neil sleep norwich human modern lack sleep separate different different human sleep closed sleep marital separate animal answered')

('NNS', 'couples beds chalabi numbers data github data datasleepingalonedata douglas censors beds beds couples donts decades beds doesnt data chances whats people arrangements data respondents couples data responses adults data respondents couples respondents stereotypes couples years ones lets factors respondents respondents reasons points couples couples beds factors years chances years years results versa turns beds respondents beds people bedrooms choices arrangements patterns people bedrooms wings people reasons boxes arguments cofounders tips couples respondents beds respondents effects responses norms hospitals animals activities times locations bedrooms adults couples arrangements couples kingdom numbers')

('VBP', 'sleep mona help github merle dear dear lucy love sleep sleep sleep look love minnesota rosenblatt afford question achieve result england sleep sleep hope help')

('VBN', 'filed shown changed seen compared compared made found applied realized posed used dedicated scheduled prioritized tested agreed disagreed linked linked linked')

('NN', 'hope movie wife motion picture association america item section list woman organization thought vulgarity offscreen journalist work reality harder gauge foundation percent poll someone didnt survey month audience help partnership union find github page half partner percent partner home night percent percent status story percentage rate cohabiting arrangement wife number percent night percent course vice idea wife setup room apart share bedroom percent percent find space point paul rosenblatt professor emeritus university wealth class role rosenblatt research sleep sharing house master couple apart place offer list option tick list percent percent percent bedroom behavior response question wife sleep euphemism link sleep network online community bressler lisa mercurio relationship sleeping sextime theory statement life result percent percent bedtime network comfort stanley doctor laboratory norfolk university link trend phenomenon arrangement space lead idea bedroom somehow connection reason mind time building sense shameembarrassment bedroom door place bedroom plenty status question send dearmona fivethirtyeightcom datalab538')

('VBD', 'required issued tucked told found married thought asked gathered married said said married partnered spent said said specified said said constrained said overstated held improved responded agreed said managed caused carried became started became')

('VB', 'risk take conform function play sleep privacy mona like')

('VBG', 'sharing happening sleeping existing living sleeping getting cohabiting sleeping sleeping remembering sleeping referring snoring thinking according sharing sleeping asking sleeping sleeping according sleeping developing sleeping')

('JJS', 'best least best least')

('VBZ', 'comes couples couples challenges suggests')

('JJR', 'older better')

Token analysis after pre-processing 
Number of tokens: 561
Number of types: 342
Type token ratio: 0.6096256684491979

