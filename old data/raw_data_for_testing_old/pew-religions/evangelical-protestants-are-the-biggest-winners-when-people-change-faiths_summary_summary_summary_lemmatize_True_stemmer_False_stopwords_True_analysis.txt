Token analysis before pre-processing 
Raw number of tokens: 38
Raw number of types: 38
Raw Type token ratio: 1.0

Filters:
Using stopwords filter = True
Using lemmatizer = True
Using stemmer = False

Used nouns, verbs and adjectives in article: 
(tokens separated by whitespace) 
('NN', 'percent gain wind share member mainlin model equilibrium conver cathol rate faith chanc look tradit group church make attractor child github state report recruit')

('NNS', 'data')

('VBN', 'born')

('JJ', 'popul')

('VBD', 'lost')

('VBZ', 'denomin')

('VB', 'take')

Token analysis after pre-processing 
Number of tokens: 30
Number of types: 30
Type token ratio: 1.0

